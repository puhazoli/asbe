{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASBE - Automatic Stopping for Batch Experiments\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to asbe.core,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "%nbdev_default_export core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import numpy as np\n",
    "\n",
    "from modAL.models.base import BaseLearner\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from typing import Union, Optional\n",
    "from copy import deepcopy\n",
    "from pylift.eval import UpliftEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def random_batch_sampling(classifier, X_pool, n2):\n",
    "    \"Randomly sample a batch from a pool of unlabaled samples\"\n",
    "    n_samples = X_pool.shape[0]\n",
    "    query_idx = np.random.choice(range(n_samples), size=n2,replace=False)\n",
    "    return X_pool[query_idx], query_idx\n",
    "\n",
    "def uncertainty_batch_sampling(classifier, X_pool, n2, **kwargs):\n",
    "    \"Select the top $n_2$ most uncertain units\"\n",
    "    ite_preds, y1_preds, y_preds = classifier.predict(X_pool, **kwargs)\n",
    "    # Calculate variance based on predicted\n",
    "    if y1_preds.shape[0] <= 1 or \\\n",
    "    len(y1_preds.shape) <= 1:\n",
    "            raise Exception(\"Not possible to calculate uncertainty when dimensions <=1 \")\n",
    "    ite_vars = np.var(classifier.estimator.y1_preds - classifier.estimator.y0_preds, axis=1)\n",
    "    query_idx = np.argsort(ite_vars)[-n2:][::-1]\n",
    "        \n",
    "    return X_pool[query_idx], query_idx\n",
    "    \n",
    "def expected_model_change_maximization(classifier, X_pool, n2, **kwargs):\n",
    "    \"\"\"\n",
    "    Implementation of EMCM for ITE - using a surrogate SGD model.\n",
    "    \"\"\"\n",
    "    # Get mean of predicted ITE\n",
    "    # First, check is needed if approx_model is fitted or not \n",
    "    ite_train_preds, y1_train_preds, y0_train_preds = \\\n",
    "        classifier.predict(classifier.X_training, **kwargs)\n",
    "    ite_pool_preds, y1_pool_preds, y0_pool_preds = \\\n",
    "        classifier.predict(X_pool, **kwargs)\n",
    "    # Then scale the data so sgd works the best\n",
    "    sc = StandardScaler()\n",
    "    X_scaled = sc.fit_transform(classifier.X_training)\n",
    "    classifier.approx_model.fit(\n",
    "        X_scaled,\n",
    "        ite_train_preds if ite_train_preds.shape[1] <= 1 else np.mean(ite_train_preds, axis=1))\n",
    "    # Pre-allocating memory\n",
    "    query_idx = []\n",
    "    for ix in range(n2):\n",
    "        # Select randomly from X_pool\n",
    "        prob_sampling = np.ones((X_pool.shape[0]))/(X_pool.shape[0]-len(query_idx))\n",
    "        if ix > 0:\n",
    "            prob_sampling[query_idx] = 0\n",
    "        considered_ixes = np.random.choice(X_pool.shape[0],\n",
    "                                         size = kwargs[\"B\"] if \"B\" in kwargs else 100,\n",
    "                                         replace=False, \n",
    "                                         p=prob_sampling)\n",
    "        grads = np.array([])\n",
    "        for considered_ix in considered_ixes:\n",
    "            new_X = sc.transform(X_pool[considered_ix].reshape(1, -1))\n",
    "            app_predicted_ite = classifier.approx_model.predict(new_X)\n",
    "            true_ite = np.random.choice(ite_pool_preds[considered_ix], size=1)\n",
    "            grad = (true_ite - app_predicted_ite)*new_X\n",
    "            grads = np.append(grads, np.sum(np.abs(grad)))\n",
    "        query_idx.append(int(considered_ixes[np.argmax(grads)]))\n",
    "        classifier.approx_model.partial_fit(\n",
    "            sc.transform(X_pool[int(query_idx[ix])].reshape(1, -1)),\n",
    "            np.random.choice(ite_pool_preds[int(query_idx[ix])], size=1))\n",
    "        \n",
    "    return X_pool[query_idx], query_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "estimator_type = ClassifierMixin\n",
    "class ASLearner(BaseLearner):\n",
    "    \"\"\"A(ctively)S(topping)Learner class for automatic stopping in batch-mode AL\"\"\"\n",
    "    def __init__(self,\n",
    "                 estimator: estimator_type=None, \n",
    "                 query_strategy=None,\n",
    "                 assignment_fc=None,\n",
    "                 X_training: np.ndarray = None,\n",
    "                 t_training: np.ndarray = None,\n",
    "                 y_training: np.ndarray = None,\n",
    "                 X_pool: np.ndarray = None,\n",
    "                 X_test: np.ndarray = None,\n",
    "                 approx_model: RegressorMixin = None\n",
    "                ) -> None:\n",
    "        self.estimator = estimator\n",
    "        self.query_strategy = query_strategy\n",
    "        self.assignment_fc = assignment_fc\n",
    "        self.X_training = X_training\n",
    "        self.y_training = y_training\n",
    "        self.t_training = t_training\n",
    "        self.X_pool     = X_pool\n",
    "        self.X_test     = X_test\n",
    "        self.approx_model = approx_model\n",
    "        \n",
    "    def _add_queried_data_class(self, X, t, y):\n",
    "        self.X_training = np.vstack((self.X_training, X))\n",
    "        self.t_training = np.concatenate((self.t_training, t))\n",
    "        self.y_training = np.concatenate((self.y_training, y))\n",
    "    \n",
    "    def _update_estimator_values(self):\n",
    "        self.estimator.__dict__.update(X_training = self.X_training,\n",
    "                               y_training  =        self.y_training,\n",
    "                               t_training  =        self.t_training,\n",
    "                               X_test      =        self.X_test)\n",
    "\n",
    "    def teach(self, X_new, t_new, y_new):\n",
    "        \"\"\"Teaching new instances to the estimator selected bu the query_strategy\n",
    "        \n",
    "        If no `assignment_fc` is added, all selected samples are used\n",
    "        If assignment function is added, only those instances are used, where\n",
    "        $\\hat{T} = T$\n",
    "        \"\"\"\n",
    "        if self.assignment_fc is None:\n",
    "            self._add_queried_data_class(X_new, t_new, y_new)\n",
    "            self.fit()\n",
    "    \n",
    "    def fit(self):\n",
    "        self._update_estimator_values()\n",
    "        self.estimator.fit()\n",
    "        \n",
    "    def predict(self, X=None, **kwargs):\n",
    "        \"\"\"Method for predicting treatment effects within Active Learning\n",
    "        \n",
    "        Default is to predict on the unlabeled pool\"\"\"\n",
    "        if X is None:\n",
    "            raise Exception(\"You need to supply an unlabeled pool of instances (with shape (-1,{}))\".format(self.X_training.shape[1]))\n",
    "        self.preds = self.estimator.predict(X, **kwargs)\n",
    "        return self.preds\n",
    "    \n",
    "    def score(self, preds=None, y_true=None, t_true=None, metric = \"Qini\"):\n",
    "        \"\"\"\n",
    "        Scoring the predictions - either ITE or observed outcomes are needed.\n",
    "        \n",
    "        If observed outcomes are provided, the accompanying treatments are also needed.\n",
    "        \"\"\"\n",
    "        if metric == \"Qini\":\n",
    "            upev = UpliftEval(t_true, y_true, self.preds[0] if preds is None else preds)\n",
    "            self.scores = upev\n",
    "        return self.scores.q1_aqini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class ITEEstimator(BaseEstimator):\n",
    "    \"\"\" Class for building a naive estimator for ITE estimation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: estimator_type = None,\n",
    "                 two_model: bool = False,\n",
    "                 **kwargs\n",
    "                ) -> None:\n",
    "        self.model = model\n",
    "        self.two_model = two_model\n",
    "\n",
    "    def fit(self,X_training: np.ndarray = None,\n",
    "                 t_training: np.ndarray = None,\n",
    "                 y_training: np.ndarray = None,\n",
    "                 X_test: np.ndarray = None):\n",
    "        if X_training is not None:\n",
    "            self.X_training = X_training\n",
    "            self.y_training = y_training\n",
    "            self.t_training = t_training\n",
    "            self.X_test = X_test\n",
    "        self.N_training = self.X_training.shape[0]\n",
    "        # if \"N_training\" not in self.__dict__:\n",
    "        #     self.N_training = self.X_training.shape[0]\n",
    "        if self.two_model:\n",
    "            self.m1 = deepcopy(self.model)\n",
    "            control_ix = np.where(self.t_training == 0)[0]\n",
    "            self.model.fit(self.X_training[control_ix,:],\n",
    "                           self.y_training[control_ix])\n",
    "            self.m1.fit(self.X_training[-control_ix,:],\n",
    "                        self.y_training[-control_ix])\n",
    "        else:\n",
    "            self.model.fit(np.hstack((self.X_training,\n",
    "                                      self.t_training.reshape((self.N_training, -1)))),\n",
    "                           self.y_training)\n",
    "            \n",
    "    def _predict_without_proba(self, model, X, **kwargs):\n",
    "        return model.predict(X,\n",
    "            return_mean = kwargs[\"return_mean\"] if \"return_mean\" in kwargs else True)\n",
    "            \n",
    "    def predict(self, X=None, **kwargs):\n",
    "        if X is None:\n",
    "            X = self.X_test\n",
    "        N_test = X.shape[0]\n",
    "        try:\n",
    "            if self.two_model:\n",
    "                self.y1_preds = self.m1.predict_proba(X)[:,1]\n",
    "                self.y0_preds = self.model.predict_proba(X)[:,1]\n",
    "            else:\n",
    "                \n",
    "                self.y1_preds = self.model.predict_proba(\n",
    "                                    np.hstack((X,\n",
    "                                    np.ones(N_test).reshape(-1,1))))[:,1]\n",
    "                self.y0_preds = self.model.predict_proba(\n",
    "                    np.hstack((X,\n",
    "                               np.zeros(N_test).reshape(-1,1))))[:,1]\n",
    "        except AttributeError:\n",
    "            if type(self.model) is XBART:\n",
    "                if self.two_model:\n",
    "                    self.y1_preds = self._predict_without_proba(self.m1, X, **kwargs)\n",
    "                    self.y0_preds = self._predict_without_proba(self.model, X, **kwargs)\n",
    "                else: \n",
    "                    self.y1_preds = self._predict_without_proba(self.model,\n",
    "                             np.hstack((X,\n",
    "                             np.ones(N_test).reshape(-1,1))), **kwargs)\n",
    "                    self.y0_preds = self._predict_without_proba(self.model,\n",
    "                        np.hstack((X,\n",
    "                                   np.zeros(N_test).reshape(-1,1))), **kwargs)\n",
    "        return self.y1_preds - self.y0_preds, self.y1_preds, self.y0_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(size = 1000).reshape((500,2))\n",
    "t = np.random.binomial(n = 1, p = 0.5, size = 500)\n",
    "y = np.random.binomial(n = 1, p = 1/(1+np.exp(X[:, 1]*2 + t*3)))\n",
    "X_test = np.random.normal(size = 200).reshape((100,2))\n",
    "t_test = np.random.binomial(n = 1, p = 0.5, size = 100)\n",
    "y_test = np.random.binomial(n = 1, p = 1/(1+np.exp(X_test[:, 1]*2 + t_test*3)))\n",
    "a = ITEEstimator(LogisticRegression(solver=\"lbfgs\"), two_model = True)\n",
    "a.fit(X, t, y)\n",
    "assert type(a.model) == LogisticRegression  # test assigning a model\n",
    "assert a.X_training.shape  == (500,2)       # test data passing for class\n",
    "assert a.model.intercept_ is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = ITEEstimator(RandomForestClassifier(), X, t, y, X_test, two_model = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "asl = ASLearner(estimator = ITEEstimator(model = RandomForestClassifier()), \n",
    "         query_strategy=random_batch_sampling,\n",
    "         X_training=X,\n",
    "                t_training=t,\n",
    "                y_training=y,\n",
    "                X_test=X_test)\n",
    "asl.fit()\n",
    "ite_pred, y1_pred, y0_pred = asl.predict(asl.X_test)\n",
    "X_sel, query_sel = asl.query(asl.X_test, n2=10)\n",
    "assert ite_pred.shape[0] == 100\n",
    "assert X_sel.shape       == (10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbart import XBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100\n",
    "p       = 5\n",
    "n_pool  = 1000\n",
    "n_test  = 1000\n",
    "n2      = 20\n",
    "X_train = np.random.normal(size = n_train*p).reshape((n_train,p))\n",
    "t_train = np.random.binomial(n = 1, p = 0.5, size = n_train)\n",
    "y_train = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_train[:, 1]*2 + t_train*3))))\n",
    "\n",
    "X_pool = np.random.normal(size = n_pool*p).reshape((n_pool,p))\n",
    "t_pool = np.random.binomial(n = 1, p = 0.5, size = n_pool)\n",
    "y_pool = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_pool[:, 1]*2 + t_pool*3))))\n",
    "\n",
    "X_test = np.random.normal(size = n_test*p).reshape((n_test,p))\n",
    "t_test = np.random.binomial(n = 1, p = 0.5, size = n_test)\n",
    "y_test = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_test[:, 1]*2 + t_test*3))))\n",
    "# asl = ASLearner(estimator = ITEEstimator(model = XBART(),two_model=False), \n",
    "#          query_strategy=uncertainty_batch_sampling,\n",
    "#          X_training = X_train,\n",
    "#          t_training = t_train,\n",
    "#          y_training = y_train,\n",
    "#          X_pool     = X_pool,\n",
    "#          X_test     = X_test)\n",
    "# asl.fit()\n",
    "# p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "# print(\"Qini before AL: {}\".format(asl.score(preds=np.mean(p_ite, axis=1),\n",
    "#                                             y_true=y_test, t_true=t_test)))\n",
    "# qini_vals = []\n",
    "# for _ in range(10):\n",
    "#     X_query, ix = asl.query(asl.X_pool, n2=n2)\n",
    "#     asl.teach(X_query, t_pool[ix], y_pool[ix])\n",
    "#     asl.X_pool = np.delete(asl.X_pool,ix, axis=0)\n",
    "#     t_pool     = np.delete(t_pool,ix, axis=0) \n",
    "#     y_pool     = np.delete(y_pool,ix, axis=0) \n",
    "#     p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "#     qini_vals.append(asl.score(preds=np.mean(p_ite, axis=1), y_true=y_test, t_true=t_test))\n",
    "#     print(\"Qini after round {} of AL: {}\".format(_,qini_vals[_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qini before AL: 0.03187445625445961\n",
      "[]\n",
      "<class 'list'>\n",
      "[871]\n",
      "<class 'list'>\n",
      "[871, 349]\n",
      "<class 'list'>\n",
      "[871, 349, 556]\n",
      "<class 'list'>\n",
      "[871, 349, 556, 962]\n",
      "<class 'list'>\n",
      "Qini after round 0 of AL: 0.04855786348688202\n",
      "[]\n",
      "<class 'list'>\n",
      "[768]\n",
      "<class 'list'>\n",
      "[768, 873]\n",
      "<class 'list'>\n",
      "[768, 873, 301]\n",
      "<class 'list'>\n",
      "[768, 873, 301, 294]\n",
      "<class 'list'>\n",
      "Qini after round 1 of AL: 0.047194229600781186\n",
      "[]\n",
      "<class 'list'>\n",
      "[379]\n",
      "<class 'list'>\n",
      "[379, 577]\n",
      "<class 'list'>\n",
      "[379, 577, 77]\n",
      "<class 'list'>\n",
      "[379, 577, 77, 473]\n",
      "<class 'list'>\n",
      "Qini after round 2 of AL: 0.07919964947315958\n",
      "[]\n",
      "<class 'list'>\n",
      "[602]\n",
      "<class 'list'>\n",
      "[602, 186]\n",
      "<class 'list'>\n",
      "[602, 186, 738]\n",
      "<class 'list'>\n",
      "[602, 186, 738, 394]\n",
      "<class 'list'>\n",
      "Qini after round 3 of AL: 0.10468973771869663\n",
      "[]\n",
      "<class 'list'>\n",
      "[374]\n",
      "<class 'list'>\n",
      "[374, 801]\n",
      "<class 'list'>\n",
      "[374, 801, 160]\n",
      "<class 'list'>\n",
      "[374, 801, 160, 15]\n",
      "<class 'list'>\n",
      "Qini after round 4 of AL: 0.04522925589663235\n",
      "[]\n",
      "<class 'list'>\n",
      "[94]\n",
      "<class 'list'>\n",
      "[94, 697]\n",
      "<class 'list'>\n",
      "[94, 697, 655]\n",
      "<class 'list'>\n",
      "[94, 697, 655, 33]\n",
      "<class 'list'>\n",
      "Qini after round 5 of AL: 0.11429427322337288\n",
      "[]\n",
      "<class 'list'>\n",
      "[631]\n",
      "<class 'list'>\n",
      "[631, 851]\n",
      "<class 'list'>\n",
      "[631, 851, 875]\n",
      "<class 'list'>\n",
      "[631, 851, 875, 110]\n",
      "<class 'list'>\n",
      "Qini after round 6 of AL: 0.0667171736935353\n",
      "[]\n",
      "<class 'list'>\n",
      "[123]\n",
      "<class 'list'>\n",
      "[123, 920]\n",
      "<class 'list'>\n",
      "[123, 920, 692]\n",
      "<class 'list'>\n",
      "[123, 920, 692, 958]\n",
      "<class 'list'>\n",
      "Qini after round 7 of AL: 0.08016000561430085\n",
      "[]\n",
      "<class 'list'>\n",
      "[840]\n",
      "<class 'list'>\n",
      "[840, 790]\n",
      "<class 'list'>\n",
      "[840, 790, 937]\n",
      "<class 'list'>\n",
      "[840, 790, 937, 531]\n",
      "<class 'list'>\n",
      "Qini after round 8 of AL: 0.10000795309701725\n",
      "[]\n",
      "<class 'list'>\n",
      "[521]\n",
      "<class 'list'>\n",
      "[521, 604]\n",
      "<class 'list'>\n",
      "[521, 604, 609]\n",
      "<class 'list'>\n",
      "[521, 604, 609, 466]\n",
      "<class 'list'>\n",
      "Qini after round 9 of AL: 0.07175940936153831\n"
     ]
    }
   ],
   "source": [
    "asl = ASLearner(estimator = ITEEstimator(model = XBART(),two_model=False), \n",
    "         query_strategy=expected_model_change_maximization,\n",
    "         X_training = X_train,\n",
    "         t_training = t_train,\n",
    "         y_training = y_train,\n",
    "         X_pool     = X_pool,\n",
    "         X_test     = X_test,\n",
    "         approx_model=SGDRegressor())\n",
    "asl.fit()\n",
    "p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "print(\"Qini before AL: {}\".format(asl.score(preds=np.mean(p_ite, axis=1),\n",
    "                                            y_true=y_test, t_true=t_test)))\n",
    "qini_vals = []\n",
    "for _ in range(10):\n",
    "    X_query, ix = asl.query(asl.X_pool, n2=5, return_mean=False)\n",
    "    asl.teach(X_query, t_pool[ix], y_pool[ix])\n",
    "    asl.X_pool = np.delete(asl.X_pool,ix, axis=0)\n",
    "    t_pool     = np.delete(t_pool,ix, axis=0) \n",
    "    y_pool     = np.delete(y_pool,ix, axis=0) \n",
    "    p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "    qini_vals.append(asl.score(preds=np.mean(p_ite, axis=1), y_true=y_test, t_true=t_test))\n",
    "    print(\"Qini after round {} of AL: {}\".format(_,qini_vals[_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asl.approx_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
