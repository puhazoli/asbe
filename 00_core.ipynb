{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASBE - Automatic Stopping for Batch Experiments\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to asbe.core,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "%nbdev_default_export core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "import numpy as np\n",
    "\n",
    "from modAL.models.base import BaseLearner\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from typing import Union, Optional\n",
    "from copy import deepcopy\n",
    "from pylift.eval import UpliftEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def random_batch_sampling(classifier, X_pool, n2):\n",
    "    \"Randomly sample a batch from a pool of unlabaled samples\"\n",
    "    n_samples = X_pool.shape[0]\n",
    "    query_idx = np.random.choice(range(n_samples), size=n2,replace=False)\n",
    "    return X_pool[query_idx], query_idx\n",
    "\n",
    "def uncertainty_batch_sampling(classifier, X_pool, n2, **kwargs):\n",
    "    \"Select the top $n_2$ most uncertain units\"\n",
    "    ite_preds, y1_preds, y_preds = classifier.predict(X_pool, **kwargs)\n",
    "    # Calculate variance based on predicted\n",
    "    if y1_preds.shape[0] <= 1 or \\\n",
    "    len(y1_preds.shape) <= 1:\n",
    "            raise Exception(\"Not possible to calculate uncertainty when dimensions <=1 \")\n",
    "    ite_vars = np.var(classifier.estimator.y1_preds - classifier.estimator.y0_preds, axis=1)\n",
    "    query_idx = np.argsort(ite_vars)[-n2:][::-1]\n",
    "        \n",
    "    return X_pool[query_idx], query_idx\n",
    "    \n",
    "def expected_model_change_maximization(classifier, X_pool, n2, **kwargs):\n",
    "    \"\"\"\n",
    "    Implementation of EMCM for ITE - using a surrogate SGD model.\n",
    "    \"\"\"\n",
    "    # Get mean of predicted ITE\n",
    "    # First, check is needed if approx_model is fitted or not \n",
    "    ite_train_preds, y1_train_preds, y0_train_preds = \\\n",
    "        classifier.predict(classifier.X_training, **kwargs)\n",
    "    ite_pool_preds, y1_pool_preds, y0_pool_preds = \\\n",
    "        classifier.predict(X_pool, **kwargs)\n",
    "    # Then scale the data so sgd works the best\n",
    "    sc = StandardScaler()\n",
    "    X_scaled = sc.fit_transform(classifier.X_training)\n",
    "    classifier.approx_model.fit(\n",
    "        X_scaled,\n",
    "        ite_train_preds if ite_train_preds.shape[1] <= 1 else np.mean(ite_train_preds, axis=1))\n",
    "    for _ in range(n2):\n",
    "        # Select randomly from X_pool\n",
    "        considered_ixes = np.random.choice(X_pool.shape[0],\n",
    "                                         size = kwargs[\"B\"] if \"B\" in kwargs else 100,\n",
    "                                         replace=False)\n",
    "        grads = np.array([])\n",
    "        for considered_ix in considered_ixes:\n",
    "            new_X = sc.transform(X_pool[considered_ix].reshape(1, -1))\n",
    "            app_predicted_ite = classifier.approx_model.predict(new_X)\n",
    "            true_ite = np.random.choice(ite_pool_preds[considered_ix], size=1)\n",
    "            grad = (true_ite - app_predicted_ite)*new_X\n",
    "            grads = np.append(grads, np.sum(np.abs(grad)))\n",
    "        print(grads)\n",
    "        print(np.argmax(grads))\n",
    "            #classifier.approx_model.partial_fit(new_X, \n",
    "            #                      new_ite)\n",
    "#\n",
    "            #classifier.approx_model = deepcopy(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "estimator_type = ClassifierMixin\n",
    "class ASLearner(BaseLearner):\n",
    "    \"\"\"A(ctively)S(topping)Learner class for automatic stopping in batch-mode AL\"\"\"\n",
    "    def __init__(self,\n",
    "                 estimator: estimator_type=None, \n",
    "                 query_strategy=None,\n",
    "                 assignment_fc=None,\n",
    "                 X_training: np.ndarray = None,\n",
    "                 t_training: np.ndarray = None,\n",
    "                 y_training: np.ndarray = None,\n",
    "                 X_pool: np.ndarray = None,\n",
    "                 X_test: np.ndarray = None,\n",
    "                 approx_model: RegressorMixin = None\n",
    "                ) -> None:\n",
    "        self.estimator = estimator\n",
    "        self.query_strategy = query_strategy\n",
    "        self.assignment_fc = assignment_fc\n",
    "        self.X_training = X_training\n",
    "        self.y_training = y_training\n",
    "        self.t_training = t_training\n",
    "        self.X_pool     = X_pool\n",
    "        self.X_test     = X_test\n",
    "        self.approx_model = approx_model\n",
    "        \n",
    "    def _add_queried_data_class(self, X, t, y):\n",
    "        self.X_training = np.vstack((self.X_training, X))\n",
    "        self.t_training = np.concatenate((self.t_training, t))\n",
    "        self.y_training = np.concatenate((self.y_training, y))\n",
    "    \n",
    "    def _update_estimator_values(self):\n",
    "        self.estimator.__dict__.update(X_training = self.X_training,\n",
    "                               y_training  =        self.y_training,\n",
    "                               t_training  =        self.t_training,\n",
    "                               X_test      =        self.X_test)\n",
    "\n",
    "    def teach(self, X_new, t_new, y_new):\n",
    "        \"\"\"Teaching new instances to the estimator selected bu the query_strategy\n",
    "        \n",
    "        If no `assignment_fc` is added, all selected samples are used\n",
    "        If assignment function is added, only those instances are used, where\n",
    "        $\\hat{T} = T$\n",
    "        \"\"\"\n",
    "        if self.assignment_fc is None:\n",
    "            self._add_queried_data_class(X_new, t_new, y_new)\n",
    "            self.fit()\n",
    "    \n",
    "    def fit(self):\n",
    "        self._update_estimator_values()\n",
    "        self.estimator.fit()\n",
    "        \n",
    "    def predict(self, X=None, **kwargs):\n",
    "        \"\"\"Method for predicting treatment effects within Active Learning\n",
    "        \n",
    "        Default is to predict on the unlabeled pool\"\"\"\n",
    "        if X is None:\n",
    "            raise Exception(\"You need to supply an unlabeled pool of instances (with shape (-1,{}))\".format(self.X_training.shape[1]))\n",
    "        self.preds = self.estimator.predict(X, **kwargs)\n",
    "        return self.preds\n",
    "    \n",
    "    def score(self, preds=None, y_true=None, t_true=None, metric = \"Qini\"):\n",
    "        \"\"\"\n",
    "        Scoring the predictions - either ITE or observed outcomes are needed.\n",
    "        \n",
    "        If observed outcomes are provided, the accompanying treatments are also needed.\n",
    "        \"\"\"\n",
    "        if metric == \"Qini\":\n",
    "            upev = UpliftEval(t_true, y_true, self.preds[0] if preds is None else preds)\n",
    "            self.scores = upev\n",
    "        return self.scores.q1_aqini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class ITEEstimator(BaseEstimator):\n",
    "    \"\"\" Class for building a naive estimator for ITE estimation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: estimator_type = None,\n",
    "                 two_model: bool = False,\n",
    "                 **kwargs\n",
    "                ) -> None:\n",
    "        self.model = model\n",
    "        self.two_model = two_model\n",
    "\n",
    "    def fit(self,X_training: np.ndarray = None,\n",
    "                 t_training: np.ndarray = None,\n",
    "                 y_training: np.ndarray = None,\n",
    "                 X_test: np.ndarray = None):\n",
    "        if X_training is not None:\n",
    "            self.X_training = X_training\n",
    "            self.y_training = y_training\n",
    "            self.t_training = t_training\n",
    "            self.X_test = X_test\n",
    "        self.N_training = self.X_training.shape[0]\n",
    "        # if \"N_training\" not in self.__dict__:\n",
    "        #     self.N_training = self.X_training.shape[0]\n",
    "        if self.two_model:\n",
    "            self.m1 = deepcopy(self.model)\n",
    "            control_ix = np.where(self.t_training == 0)[0]\n",
    "            self.model.fit(self.X_training[control_ix,:],\n",
    "                           self.y_training[control_ix])\n",
    "            self.m1.fit(self.X_training[-control_ix,:],\n",
    "                        self.y_training[-control_ix])\n",
    "        else:\n",
    "            self.model.fit(np.hstack((self.X_training,\n",
    "                                      self.t_training.reshape((self.N_training, -1)))),\n",
    "                           self.y_training)\n",
    "            \n",
    "    def _predict_without_proba(self, model, X, **kwargs):\n",
    "        return model.predict(X,\n",
    "            return_mean = kwargs[\"return_mean\"] if \"return_mean\" in kwargs else True)\n",
    "            \n",
    "    def predict(self, X=None, **kwargs):\n",
    "        if X is None:\n",
    "            X = self.X_test\n",
    "        N_test = X.shape[0]\n",
    "        try:\n",
    "            if self.two_model:\n",
    "                self.y1_preds = self.m1.predict_proba(X)[:,1]\n",
    "                self.y0_preds = self.model.predict_proba(X)[:,1]\n",
    "            else:\n",
    "                \n",
    "                self.y1_preds = self.model.predict_proba(\n",
    "                                    np.hstack((X,\n",
    "                                    np.ones(N_test).reshape(-1,1))))[:,1]\n",
    "                self.y0_preds = self.model.predict_proba(\n",
    "                    np.hstack((X,\n",
    "                               np.zeros(N_test).reshape(-1,1))))[:,1]\n",
    "        except AttributeError:\n",
    "            if type(self.model) is XBART:\n",
    "                if self.two_model:\n",
    "                    self.y1_preds = self._predict_without_proba(self.m1, X, **kwargs)\n",
    "                    self.y0_preds = self._predict_without_proba(self.model, X, **kwargs)\n",
    "                else: \n",
    "                    self.y1_preds = self._predict_without_proba(self.model,\n",
    "                             np.hstack((X,\n",
    "                             np.ones(N_test).reshape(-1,1))), **kwargs)\n",
    "                    self.y0_preds = self._predict_without_proba(self.model,\n",
    "                        np.hstack((X,\n",
    "                                   np.zeros(N_test).reshape(-1,1))), **kwargs)\n",
    "        return self.y1_preds - self.y0_preds, self.y1_preds, self.y0_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(size = 1000).reshape((500,2))\n",
    "t = np.random.binomial(n = 1, p = 0.5, size = 500)\n",
    "y = np.random.binomial(n = 1, p = 1/(1+np.exp(X[:, 1]*2 + t*3)))\n",
    "X_test = np.random.normal(size = 200).reshape((100,2))\n",
    "t_test = np.random.binomial(n = 1, p = 0.5, size = 100)\n",
    "y_test = np.random.binomial(n = 1, p = 1/(1+np.exp(X_test[:, 1]*2 + t_test*3)))\n",
    "a = ITEEstimator(LogisticRegression(solver=\"lbfgs\"), two_model = True)\n",
    "a.fit(X, t, y)\n",
    "assert type(a.model) == LogisticRegression  # test assigning a model\n",
    "assert a.X_training.shape  == (500,2)       # test data passing for class\n",
    "assert a.model.intercept_ is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = ITEEstimator(RandomForestClassifier(), X, t, y, X_test, two_model = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "asl = ASLearner(estimator = ITEEstimator(model = RandomForestClassifier()), \n",
    "         query_strategy=random_batch_sampling,\n",
    "         X_training=X,\n",
    "                t_training=t,\n",
    "                y_training=y,\n",
    "                X_test=X_test)\n",
    "asl.fit()\n",
    "ite_pred, y1_pred, y0_pred = asl.predict(asl.X_test)\n",
    "X_sel, query_sel = asl.query(asl.X_test, n2=10)\n",
    "assert ite_pred.shape[0] == 100\n",
    "assert X_sel.shape       == (10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbart import XBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 100\n",
    "p       = 5\n",
    "n_pool  = 1000\n",
    "n_test  = 1000\n",
    "n2      = 20\n",
    "X_train = np.random.normal(size = n_train*p).reshape((n_train,p))\n",
    "t_train = np.random.binomial(n = 1, p = 0.5, size = n_train)\n",
    "y_train = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_train[:, 1]*2 + t_train*3))))\n",
    "\n",
    "X_pool = np.random.normal(size = n_pool*p).reshape((n_pool,p))\n",
    "t_pool = np.random.binomial(n = 1, p = 0.5, size = n_pool)\n",
    "y_pool = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_pool[:, 1]*2 + t_pool*3))))\n",
    "\n",
    "X_test = np.random.normal(size = n_test*p).reshape((n_test,p))\n",
    "t_test = np.random.binomial(n = 1, p = 0.5, size = n_test)\n",
    "y_test = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_test[:, 1]*2 + t_test*3))))\n",
    "# asl = ASLearner(estimator = ITEEstimator(model = XBART(),two_model=False), \n",
    "#          query_strategy=uncertainty_batch_sampling,\n",
    "#          X_training = X_train,\n",
    "#          t_training = t_train,\n",
    "#          y_training = y_train,\n",
    "#          X_pool     = X_pool,\n",
    "#          X_test     = X_test)\n",
    "# asl.fit()\n",
    "# p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "# print(\"Qini before AL: {}\".format(asl.score(preds=np.mean(p_ite, axis=1),\n",
    "#                                             y_true=y_test, t_true=t_test)))\n",
    "# qini_vals = []\n",
    "# for _ in range(10):\n",
    "#     X_query, ix = asl.query(asl.X_pool, n2=n2)\n",
    "#     asl.teach(X_query, t_pool[ix], y_pool[ix])\n",
    "#     asl.X_pool = np.delete(asl.X_pool,ix, axis=0)\n",
    "#     t_pool     = np.delete(t_pool,ix, axis=0) \n",
    "#     y_pool     = np.delete(y_pool,ix, axis=0) \n",
    "#     p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "#     qini_vals.append(asl.score(preds=np.mean(p_ite, axis=1), y_true=y_test, t_true=t_test))\n",
    "#     print(\"Qini after round {} of AL: {}\".format(_,qini_vals[_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qini before AL: 0.09431154962256037\n",
      "[0.21124305 0.21598287 0.09501379 0.09647031 0.26614258 0.49865082\n",
      " 0.16539172 0.44085052 0.36188786 1.1714934  0.79272894 0.39879431\n",
      " 0.93410604 0.24836542 0.90346257 0.5476222  0.46429224 0.26041212\n",
      " 0.19946694 0.58020602 0.40478143 0.19330525 0.95300293 0.02416049\n",
      " 1.36075768 0.92113204 0.51178662 0.29277958 0.95460937 0.70023424\n",
      " 0.21809775 0.54393716 0.8655763  0.01828382 1.03399455 0.12860417\n",
      " 0.3799757  0.25799289 0.57360224 0.76699683 0.50035066 0.66963189\n",
      " 0.83272655 0.03119917 0.40349528 0.58537266 0.2888242  0.48064031\n",
      " 0.28004254 0.23403841 0.20094211 0.13266484 0.8093871  0.41480889\n",
      " 0.26334139 0.60165627 0.08744129 0.23143491 0.3904686  0.19273912\n",
      " 0.12224409 0.12718273 0.37703751 0.47923466 0.57030385 0.05010724\n",
      " 0.52917161 0.01320154 0.01751916 0.33818789 1.65077674 1.52260627\n",
      " 0.38357892 0.59769288 0.68951368 0.17691678 0.93108315 0.25146054\n",
      " 1.04200107 0.36624004 0.29206165 0.45317848 0.50133298 0.09862385\n",
      " 0.10653726 1.62904009 0.6779904  0.23190659 0.00639558 0.11638013\n",
      " 1.48790802 0.76975225 0.19955104 0.47562031 0.00517451 0.15347633\n",
      " 0.61257516 0.73948229 0.41738826 0.18002063]\n",
      "70\n",
      "[0.12880248 0.07142751 0.41993316 1.5732667  0.21372161 0.48879496\n",
      " 0.02837347 0.35905133 1.08064133 0.02397945 0.13171885 0.34597936\n",
      " 0.19252046 0.48195437 0.32674543 1.02177202 0.72628343 0.05390644\n",
      " 0.75629115 0.15477001 0.69177928 0.37988705 1.04315143 0.43653384\n",
      " 0.37260417 0.2275542  1.031366   0.0200937  0.93873284 0.05228969\n",
      " 0.24270696 0.46167904 0.66040808 0.43444743 0.94519922 0.66995055\n",
      " 0.32336016 0.42803299 0.32458477 0.11210422 0.43874499 0.19655407\n",
      " 0.99096744 0.80586241 0.35294057 0.52754068 0.29281693 0.02652868\n",
      " 0.30394063 0.54391608 0.47092426 0.09208121 0.6115769  0.51427092\n",
      " 1.03708989 0.557986   0.19404473 0.78649093 0.56011138 0.70317341\n",
      " 0.64627601 1.96651132 0.92858219 0.53811146 0.80535311 0.44229253\n",
      " 0.56813171 0.35134588 0.29920194 0.16315473 0.10725709 1.12028031\n",
      " 1.22037023 0.31191239 0.04492357 0.21446796 1.40917802 0.30352125\n",
      " 2.42451037 0.00866221 0.61573404 1.05159501 0.41652091 0.07906777\n",
      " 0.7481303  0.30176459 0.77653728 0.35175252 0.0399844  0.09186964\n",
      " 1.23579428 0.29224395 0.50443894 0.02230423 0.18274478 0.15454248\n",
      " 0.30672293 0.58470678 0.09535652 0.04254686]\n",
      "78\n",
      "[0.28589339 0.22037938 0.21401214 0.62119024 0.8941234  0.02997103\n",
      " 0.38736205 0.6416882  0.58647568 0.14163592 0.32968656 0.21981308\n",
      " 0.16586195 0.36192525 0.64041636 1.20657954 0.03784174 0.34463504\n",
      " 0.24795917 0.53831996 0.63724751 0.35706215 0.24399126 0.01723186\n",
      " 1.06263688 0.94178481 0.24910546 0.25143803 0.31959364 0.30237809\n",
      " 0.35695039 0.74409359 0.12998403 0.07510511 1.09762522 0.53007719\n",
      " 0.25603407 0.99659619 0.81867819 0.6973289  1.05926945 0.23637773\n",
      " 0.49013892 0.34491069 0.30337361 0.22876463 0.23526703 0.04754421\n",
      " 0.36623582 0.12047049 1.66999567 0.21183935 0.0112341  0.20603634\n",
      " 0.27818439 0.22351925 0.81527611 1.14596371 0.17403985 0.15703105\n",
      " 1.06400915 0.48444116 0.1004513  0.2622242  1.80969947 0.21981528\n",
      " 0.0225552  0.06106548 0.31544852 0.7888794  0.61162023 0.97501073\n",
      " 0.35276732 0.4003868  0.08120386 0.32516802 0.29520324 0.22192675\n",
      " 0.31910338 0.01749133 0.3217446  0.35655743 2.61734128 0.16092527\n",
      " 0.73257761 0.62677665 0.18642439 0.49464146 1.0862172  0.42862797\n",
      " 1.17966075 0.08841194 0.74471389 0.01342964 0.37251565 0.04214425\n",
      " 1.32917614 1.58675239 0.44017947 0.31222153]\n",
      "82\n",
      "[0.33746873 0.25990185 0.37143236 0.17922685 1.21668561 0.12617567\n",
      " 0.23878295 1.01384393 0.8411927  0.42648648 0.1942189  0.36479568\n",
      " 1.08983667 0.88908696 0.19931298 0.53743491 0.90100546 1.24569018\n",
      " 0.796417   0.21852061 0.08342917 0.00771049 1.38454078 0.18245957\n",
      " 0.22574646 0.20827429 1.6348964  0.45664407 0.56708331 0.23700895\n",
      " 0.01641396 0.35860794 1.31787075 0.12578222 0.19298537 0.84024509\n",
      " 0.27390726 0.28130336 0.46158792 0.73011675 0.39389223 0.21807081\n",
      " 0.28400294 0.6400966  0.01520113 1.44854621 0.17574261 0.5162489\n",
      " 0.50523813 0.39472748 0.1782446  0.2738082  0.07109249 0.08353054\n",
      " 0.12345477 0.62006304 0.49072032 0.10473445 0.55190265 0.15261623\n",
      " 0.34231509 0.07440424 0.47896611 0.18504189 0.42354729 0.28112041\n",
      " 1.08831997 0.48196334 0.35903807 0.34503155 0.12090613 1.38687367\n",
      " 0.0076269  0.33621313 0.19252046 1.7715089  0.43938338 0.86801282\n",
      " 0.82398496 1.00888066 0.23988579 0.76325788 0.11311252 0.460744\n",
      " 0.70781068 0.47583678 0.60765152 0.10316182 0.71203513 0.33995208\n",
      " 0.75412564 0.13766392 0.08353251 0.34025454 0.04832626 1.11656523\n",
      " 0.10164254 0.1773932  0.28596465 0.21249427]\n",
      "75\n",
      "[0.74046922 1.43752005 0.38513676 2.34983367 0.34992615 1.23605795\n",
      " 0.29027118 0.08794383 0.00925842 0.23447532 0.36734735 0.4378385\n",
      " 0.10393345 0.6624067  0.05095432 1.37527513 0.24824444 1.51296078\n",
      " 0.1015323  0.30670409 0.45891843 0.94017008 0.21342476 0.66568548\n",
      " 0.57142675 1.03087509 1.27614767 1.39679816 0.36593254 0.78672348\n",
      " 1.25867157 0.11222845 1.30709341 0.12126012 1.15733938 0.75251912\n",
      " 0.81839964 0.83553963 0.35783214 0.20010928 0.51404762 0.18741578\n",
      " 1.19124598 0.29939616 0.66642125 0.10270117 0.26984726 0.11777616\n",
      " 1.11531965 0.43893285 0.77812461 0.33414905 0.53631043 1.12815831\n",
      " 0.3825345  0.00287455 1.80981889 0.39155112 1.07744164 0.4430262\n",
      " 0.32363152 0.4715753  0.50965215 0.53731389 0.84163746 0.61623339\n",
      " 0.33381095 0.39267417 0.67195923 0.73548428 0.36858655 0.00314482\n",
      " 0.25196967 0.36160314 0.12230162 0.75302113 1.41344311 0.32559134\n",
      " 0.24647083 0.35502466 0.59408199 1.0351903  1.20751397 1.21851836\n",
      " 0.86238848 0.09811521 0.05139336 0.1733782  0.9464843  0.3127067\n",
      " 1.00421617 0.29876972 0.3426091  0.12927778 0.42891572 0.00726413\n",
      " 1.2330949  0.31495115 0.16412601 0.13253044]\n",
      "3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-498-e45f94199db6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mqini_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mX_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0masl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0masl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "asl = ASLearner(estimator = ITEEstimator(model = XBART(),two_model=False), \n",
    "         query_strategy=expected_model_change_maximization,\n",
    "         X_training = X_train,\n",
    "         t_training = t_train,\n",
    "         y_training = y_train,\n",
    "         X_pool     = X_pool,\n",
    "         X_test     = X_test,\n",
    "         approx_model=SGDRegressor())\n",
    "asl.fit()\n",
    "p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "print(\"Qini before AL: {}\".format(asl.score(preds=np.mean(p_ite, axis=1),\n",
    "                                            y_true=y_test, t_true=t_test)))\n",
    "qini_vals = []\n",
    "for _ in range(10):\n",
    "    X_query, ix = asl.query(asl.X_pool, n2=5, return_mean=False)\n",
    "    asl.teach(X_query, t_pool[ix], y_pool[ix])\n",
    "    asl.X_pool = np.delete(asl.X_pool,ix, axis=0)\n",
    "    t_pool     = np.delete(t_pool,ix, axis=0) \n",
    "    y_pool     = np.delete(y_pool,ix, axis=0) \n",
    "    p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "    qini_vals.append(asl.score(preds=np.mean(p_ite, axis=1), y_true=y_test, t_true=t_test))\n",
    "    print(\"Qini after round {} of AL: {}\".format(_,qini_vals[_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-399-744b68f30415>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-399-744b68f30415>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    asl.approx_model.\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "asl.approx_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
