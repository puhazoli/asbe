{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASBE - Automatic Stopping for Batch Experiments\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to asbe.core,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "#default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "\n",
    "from modAL.models.base import BaseLearner\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from typing import Union, Optional, Callable\n",
    "from copy import deepcopy\n",
    "from pylift.eval import UpliftEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def random_batch_sampling(classifier, X_pool, n2, **kwargs):\n",
    "    \"Randomly sample a batch from a pool of unlabaled samples\"\n",
    "    n_samples = X_pool.shape[0]\n",
    "    query_idx = np.random.choice(range(n_samples), size=n2,replace=False)\n",
    "    return X_pool[query_idx], query_idx\n",
    "\n",
    "def uncertainty_batch_sampling(classifier, X_pool, n2, **kwargs):\n",
    "    \"Select the top $n_2$ most uncertain units\"\n",
    "    ite_preds, y1_preds, y_preds = classifier.predict(X_pool, **kwargs)\n",
    "    # Calculate variance based on predicted\n",
    "    if y1_preds.shape[0] <= 1 or \\\n",
    "    len(y1_preds.shape) <= 1:\n",
    "            raise Exception(\"Not possible to calculate uncertainty when dimensions <=1 \")\n",
    "    ite_vars = np.var(classifier.estimator.y1_preds - classifier.estimator.y0_preds, axis=1)\n",
    "    query_idx = np.argsort(ite_vars)[-n2:][::-1]\n",
    "        \n",
    "    return X_pool[query_idx], query_idx\n",
    "\n",
    "def type_s_batch_sampling(classifier, X_pool, n2, **kwargs):\n",
    "    \"Select highest type-s\"\n",
    "    ite_preds, y1_preds, y_preds = classifier.predict(X_pool, **kwargs)\n",
    "    prob_s = np.sum(ite_preds > 0, axis=1)/ite_preds.shape[1]\n",
    "    prob_s_sel = np.where(prob_s > 0.5, 1-prob_s, prob_s) + .0001\n",
    "    query_idx = np.argsort(prob_s_sel)[-n2:][::-1]\n",
    "    \n",
    "    return X_pool[query_idx], query_idx\n",
    "   \n",
    "\n",
    "def expected_model_change_maximization(classifier, X_pool, n2, **kwargs):\n",
    "    \"\"\"\n",
    "    Implementation of EMCM for ITE - using a surrogate SGD model.\n",
    "    \"\"\"\n",
    "    # Get mean of the trained prediction\n",
    "    ite_train_preds, y1_train_preds, y0_train_preds = \\\n",
    "        classifier.predict(classifier.X_training, **kwargs)\n",
    "    if ite_train_preds.shape[1] < 1:\n",
    "        raise ValueError(\"The treatment effect does not have uncertainty around it - \\\n",
    "                         consider using a different estimator\")\n",
    "    # Get mean of predicted ITE\n",
    "    ite_pool_preds, y1_pool_preds, y0_pool_preds = \\\n",
    "        classifier.predict(X_pool, **kwargs)\n",
    "    # Then scale the data so sgd works the best\n",
    "    sc = StandardScaler()\n",
    "    X_scaled = sc.fit_transform(classifier.X_training)\n",
    "    # Fit approx model\n",
    "    # calc type-s error\n",
    "    train_type_s_prob_1 = np.sum(ite_train_preds > 0, axis=1)/ite_train_preds.shape[1]\n",
    "    train_type_s = np.where(train_type_s_prob_1 > 0.5, 1-train_type_s_prob_1, train_type_s_prob_1) + .0001\n",
    "    pool_type_s_prob_1 = np.sum(ite_pool_preds > 0, axis=1)/ite_pool_preds.shape[1]\n",
    "    pool_type_s = np.where(pool_type_s_prob_1 > 0.5, 1-pool_type_s_prob_1, pool_type_s_prob_1) + .0001\n",
    "    classifier.approx_model.fit(\n",
    "        X = X_scaled,\n",
    "        y = np.mean(ite_train_preds, axis=1),\n",
    "        sample_weight = 5*train_type_s)\n",
    "    # Using list as it is faster than appending to np array\n",
    "    query_idx = []\n",
    "    # Using a loop for the combinatorial opt. part\n",
    "    for ix in range(n2):\n",
    "        if n2 > (X_pool.shape[0]):\n",
    "            raise IndexError(\"Too many samples are queried from the pool ($n_2 > ||X_pool||$)\")\n",
    "        # Select randomly from X_pool\n",
    "        prob_sampling = np.ones((X_pool.shape[0]))/(X_pool.shape[0]-len(query_idx))\n",
    "        # Set the probability of already selected samples to 0\n",
    "        if ix > 0:\n",
    "            prob_sampling[query_idx] = 0\n",
    "        # B = 100 by default, can be modified by kwargs\n",
    "        considered_ixes = np.random.choice(X_pool.shape[0],\n",
    "                                         size = kwargs[\"B\"] if \"B\" in kwargs else 100,\n",
    "                                         replace=False, \n",
    "                                         p=prob_sampling)\n",
    "        # Calculate the grads for all the \n",
    "        grads = np.array([])\n",
    "        for considered_ix in considered_ixes:\n",
    "            new_X = sc.transform(X_pool[considered_ix].reshape(1, -1))\n",
    "            app_predicted_ite = classifier.approx_model.predict(new_X)\n",
    "            # bootstrapping accroding to eq. 11 of Cai and Zhang\n",
    "            true_ite = np.random.choice(ite_pool_preds[considered_ix],\n",
    "                                        size=kwargs[\"K\"] if \"K\" in kwargs else 5)\n",
    "            grad = np.sum(np.abs(np.kron((true_ite - app_predicted_ite),new_X)))\n",
    "            grads = np.append(grads, grad)\n",
    "        if np.max(grads) < kwargs[\"threshold\"] if \"threshold\" in kwargs else 0:\n",
    "            break\n",
    "        classifier.model_change = np.append(classifier.model_change,np.max(grads))\n",
    "        query_idx.append(int(considered_ixes[np.argmax(grads)]))\n",
    "        classifier.approx_model.partial_fit(\n",
    "            sc.transform(X_pool[int(query_idx[ix])].reshape(1, -1)),\n",
    "            np.random.choice(ite_pool_preds[int(query_idx[ix])], size=1),\n",
    "            sample_weight = np.array(pool_type_s[int(query_idx[ix])]).ravel())\n",
    "        \n",
    "    return X_pool[query_idx], query_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %nbdev_export\n",
    "# def variance_assignment(classifier, X_pool, n2, **kwargs):\n",
    "#     \"\"\"Function to assign treatment or control based on the variance of the cf\n",
    "#     \"\"\"\n",
    "#     ite_pool_preds, y1_pool_preds, y0_pool_preds = \\\n",
    "#         classifier.predict(X_pool, **kwargs)\n",
    "#     var_y1 = np.var(y1_pool_preds, axis=1)\n",
    "#     var_y0 = np.var(y0_pool_preds, axis=1)\n",
    "#     prob_of_treatment = var_y1/(var_y1+var_y0)\n",
    "#     drawn_treatment = np.random.binomial(1, prob_of_treatment)\n",
    "#     return drawn_treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "estimator_type = ClassifierMixin\n",
    "class ASLearner(BaseLearner):\n",
    "    \"\"\"A(ctively)S(topping)Learner class for automatic stopping in batch-mode AL\"\"\"\n",
    "    def __init__(self,\n",
    "                 estimator: estimator_type=None, \n",
    "                 query_strategy=None,\n",
    "                 assignment_fc=None,\n",
    "                 X_training: np.ndarray = None,\n",
    "                 t_training: np.ndarray = None,\n",
    "                 y_training: np.ndarray = None,\n",
    "                 X_pool: np.ndarray = None,\n",
    "                 X_test: np.ndarray = None,\n",
    "                 approx_model: RegressorMixin = None\n",
    "                ) -> None:\n",
    "        self.estimator = estimator\n",
    "        self.query_strategy = query_strategy\n",
    "        self.assignment_fc = assignment_fc\n",
    "        self.X_training = X_training\n",
    "        self.y_training = y_training\n",
    "        self.t_training = t_training\n",
    "        self.X_pool     = X_pool\n",
    "        self.X_test     = X_test\n",
    "        self.approx_model = approx_model\n",
    "        self.model_change = np.array([])\n",
    "        \n",
    "    def _add_queried_data_class(self, X, t, y):\n",
    "        self.X_training = np.vstack((self.X_training, X))\n",
    "        self.t_training = np.concatenate((self.t_training, t))\n",
    "        self.y_training = np.concatenate((self.y_training, y))\n",
    "    \n",
    "    def _update_estimator_values(self):\n",
    "        self.estimator.__dict__.update(X_training = self.X_training,\n",
    "                               y_training  =        self.y_training,\n",
    "                               t_training  =        self.t_training,\n",
    "                               X_test      =        self.X_test)\n",
    "\n",
    "    def teach(self, X_new, t_new, y_new, **kwargs):\n",
    "        \"\"\"Teaching new instances to the estimator selected bu the query_strategy\n",
    "        \n",
    "        If no `assignment_fc` is added, all selected samples are used\n",
    "        If assignment function is added, only those instances are used, where\n",
    "        $\\hat{T} = T$\n",
    "        \"\"\"\n",
    "        if self.assignment_fc is not None:\n",
    "            X_new, t_new, y_new = self.assignment_fc(\n",
    "                self, X_new, t_new,\n",
    "                y_new, simulated=kwargs[\"simulated\"] if \"simulated\" in kwargs else False)\n",
    "        else:\n",
    "            try:\n",
    "                y_new = np.take_along_axis(y_new, t_new[:, None], axis=1)\n",
    "            except:\n",
    "                pass\n",
    "        self._add_queried_data_class(X_new, t_new.ravel(), y_new.ravel())\n",
    "        self.fit()\n",
    "\n",
    "    def fit(self):\n",
    "        self._update_estimator_values()\n",
    "        self.estimator.fit()\n",
    "        \n",
    "    def predict(self, X=None, **kwargs):\n",
    "        \"\"\"Method for predicting treatment effects within Active Learning\n",
    "        \n",
    "        Default is to predict on the unlabeled pool\"\"\"\n",
    "        if X is None:\n",
    "            raise Exception(\"You need to supply an unlabeled pool of instances (with shape (-1,{}))\".format(self.X_training.shape[1]))\n",
    "        self.preds = self.estimator.predict(X, **kwargs)\n",
    "        return self.preds\n",
    "    \n",
    "    def score(self, preds=None, y_true=None, t_true=None, metric = \"Qini\"):\n",
    "        \"\"\"\n",
    "        Scoring the predictions - either ITE or observed outcomes are needed.\n",
    "        \n",
    "        If observed outcomes are provided, the accompanying treatments are also needed.\n",
    "        \"\"\"\n",
    "        if metric == \"Qini\":\n",
    "            upev = UpliftEval(t_true, y_true, self.preds[0] if preds is None else preds)\n",
    "            self.scores = upev\n",
    "            vscore = self.scores.q1_aqini\n",
    "        if metric == \"PEHE\":\n",
    "            vscore = np.sqrt(np.mean(np.square(preds - y_true)))\n",
    "        return vscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ITEEstimator(BaseEstimator):\n",
    "    \"\"\" Class for building a naive estimator for ITE estimation\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model: estimator_type = None,\n",
    "                 two_model: bool = False,\n",
    "                 ps: Callable = None,\n",
    "                 **kwargs\n",
    "                ) -> None:\n",
    "        self.model = model\n",
    "        self.two_model = two_model\n",
    "        self.ps_model = ps\n",
    "        \n",
    "    def _fit_ps_model(self):\n",
    "        if self.ps_model is not None:\n",
    "            self.ps_model.fit(self.X_training, self.t_training)\n",
    "\n",
    "    def fit(self,X_training: np.ndarray = None,\n",
    "                 t_training: np.ndarray = None,\n",
    "                 y_training: np.ndarray = None,\n",
    "                 X_test: np.ndarray = None,\n",
    "                 ps_scores: np.ndarray = None):\n",
    "            \n",
    "        if X_training is not None:\n",
    "            self.X_training = X_training\n",
    "            self.y_training = y_training\n",
    "            self.t_training = t_training\n",
    "            self.X_test = X_test\n",
    "        self.N_training = self.X_training.shape[0]\n",
    "        try:\n",
    "            self._fit_ps_model()\n",
    "            ps_scores = self.ps_model.predict_proba(self.X_training)\n",
    "        except:\n",
    "            ps_scores = None\n",
    "            # if \"N_training\" not in self.__dict__:\n",
    "        #     self.N_training = self.X_training.shape[0]\n",
    "        if ps_scores is not None:\n",
    "            X_to_train_on = np.hstack((self.X_training, ps_scores[:,1].reshape((-1, 1))))\n",
    "        else:\n",
    "            X_to_train_on = self.X_training                \n",
    "        if self.two_model:\n",
    "            if hasattr(self, \"m1\") is False:\n",
    "                self.m1 = deepcopy(self.model)\n",
    "            control_ix = np.where(self.t_training == 0)[0]\n",
    "            self.model.fit(X_to_train_on[control_ix,:],\n",
    "                           self.y_training[control_ix])\n",
    "            self.m1.fit(X_to_train_on[-control_ix,:],\n",
    "                        self.y_training[-control_ix])\n",
    "        else:\n",
    "            self.model.fit(np.hstack((X_to_train_on,\n",
    "                                      self.t_training.reshape((self.N_training, -1)))),\n",
    "                           self.y_training)\n",
    "            \n",
    "    def _predict_without_proba(self, model, X, **kwargs):\n",
    "        return model.predict(X,\n",
    "            return_mean = kwargs[\"return_mean\"] if \"return_mean\" in kwargs else True)\n",
    "    \n",
    "    def _fix_dim_pred(self, preds):\n",
    "        pred_length = preds.shape[0]\n",
    "        if preds.shape[1] == 1:\n",
    "            if np.all(preds == 0):\n",
    "                preds = np.hstack((preds, np.ones(pred_length).reshape((-1,1))))\n",
    "            elif np.all(preds == 1): \n",
    "                preds = np.hstack((preds, np.zeros(pred_length).reshape((-1,1))))\n",
    "            preds = preds[:, ]\n",
    "        return preds         \n",
    "    \n",
    "    def predict(self, X=None, **kwargs):\n",
    "        if X is None:\n",
    "            X = self.X_test\n",
    "        if self.ps_model is not None and self.ps_model.coef_ is not None:\n",
    "            pred_ps_scores = self.ps_model.predict_proba(X)[:, 1]\n",
    "            X = np.hstack((X, pred_ps_scores.reshape(-1, 1)))\n",
    "        N_test = X.shape[0]\n",
    "        try:\n",
    "            if self.two_model:\n",
    "                self.y1_preds = self.m1.predict_proba(X)\n",
    "                self.y0_preds = self.model.predict_proba(X)\n",
    "            else:\n",
    "                self.y1_preds = self.model.predict_proba(\n",
    "                                    np.hstack((X,\n",
    "                                    np.ones(N_test).reshape(-1,1))))\n",
    "                self.y0_preds = self.model.predict_proba(\n",
    "                    np.hstack((X,\n",
    "                               np.zeros(N_test).reshape(-1,1))))\n",
    "            self.y1_preds = self._fix_dim_pred( self.y1_preds)\n",
    "            self.y0_preds = self._fix_dim_pred( self.y0_preds)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                if self.two_model:\n",
    "                    self.y1_preds = self._predict_without_proba(self.m1, X, **kwargs)\n",
    "                    self.y0_preds = self._predict_without_proba(self.model, X, **kwargs)\n",
    "                else: \n",
    "                    self.y1_preds = self._predict_without_proba(self.model,\n",
    "                             np.hstack((X,\n",
    "                             np.ones(N_test).reshape(-1,1))), **kwargs)\n",
    "                    self.y0_preds = self._predict_without_proba(self.model,\n",
    "                        np.hstack((X,\n",
    "                                   np.zeros(N_test).reshape(-1,1))), **kwargs)\n",
    "            except:\n",
    "                raise AttributeError(\"No method found for predicting with the supplied class\")\n",
    "        return self.y1_preds - self.y0_preds, self.y1_preds, self.y0_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.normal(size = 1000).reshape((500,2))\n",
    "t = np.random.binomial(n = 1, p = 0.5, size = 500)\n",
    "y = np.random.binomial(n = 1, p = 1/(1+np.exp(X[:, 1]*2 + t*3)))\n",
    "X_test = np.random.normal(size = 200).reshape((100,2))\n",
    "t_test = np.random.binomial(n = 1, p = 0.5, size = 100)\n",
    "y_test = np.random.binomial(n = 1, p = 1/(1+np.exp(X_test[:, 1]*2 + t_test*3)))\n",
    "a = ITEEstimator(LogisticRegression(solver=\"lbfgs\"), two_model = True, ps=LogisticRegression())\n",
    "a.fit(X, t, y)\n",
    "assert type(a.model) == LogisticRegression  # test assigning a model\n",
    "assert a.X_training.shape  == (500,2)       # test data passing for class\n",
    "assert a.model.intercept_ is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = ITEEstimator(RandomForestClassifier(), X, t, y, X_test, two_model = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def variance_based_assf(classifier, X, t, y, simulated=False):\n",
    "    ite_preds, y1_preds, y0_preds = classifier.predict(X, return_mean=False)\n",
    "    if len(y1_preds.shape) <= 1:\n",
    "            raise ValueError(\"Not possible to calculate variance with dim {}\".format(y1_preds.shape))\n",
    "    prop_score = np.var(y1_preds,axis=1)/(\n",
    "        np.var(y1_preds, axis=1)+np.var(y0_preds,axis=1))\n",
    "    t_assigned = np.random.binomial(1, prop_score)\n",
    "    if simulated:\n",
    "        try:\n",
    "            y = np.take_along_axis(y, t_assigned[:, None], axis=1)\n",
    "            t = t_assigned\n",
    "            usable_units = np.repeat(True, repeats=X.shape[0])\n",
    "        except:\n",
    "            raise ValueError(\"Potential outcomes are needed in a matrix with shape (n,2)\")\n",
    "    else:\n",
    "        usable_units = np.where(t_assigned == t)\n",
    "    return X[usable_units], t[usable_units], y[usable_units]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbart import XBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asl = ASLearner(estimator = ITEEstimator(model = XBART()), \n",
    "         query_strategy=random_batch_sampling,\n",
    "                assignment_fc=variance_based_assf,\n",
    "                X_training=X,\n",
    "                t_training=t,\n",
    "                y_training=y,\n",
    "                X_test=X_test)\n",
    "asl.fit()\n",
    "ite_pred, y1_pred, y0_pred = asl.predict(asl.X_test)\n",
    "X_sel, query_sel = asl.query(asl.X_test, n2=10)\n",
    "asl.teach(X_sel, t_test[query_sel], y_test[query_sel])\n",
    "assert ite_pred.shape[0] == 100\n",
    "assert X_sel.shape       == (10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.08178912, -0.09570422, -0.48961708, -0.08521253, -0.48338074,\n",
       "       -0.16149747, -0.48354472, -0.46430897, -0.54538151, -0.10013499,\n",
       "       -0.46355659, -0.15147128, -0.48792031, -0.45478962, -0.18321495,\n",
       "       -0.08830742, -0.45742293, -0.49161817, -0.57195395, -0.45047163,\n",
       "       -0.46635714, -0.39123347, -0.20259508, -0.41881421, -0.15502709,\n",
       "       -0.14932396, -0.16829883, -0.41912065, -0.08973207, -0.15900248,\n",
       "       -0.47903894, -0.44963399, -0.34601501, -0.46389063, -0.35386757,\n",
       "       -0.08129961, -0.08129961, -0.42081052, -0.120353  , -0.24373956,\n",
       "       -0.56775525, -0.43618272, -0.0859698 , -0.5036873 , -0.15900248,\n",
       "       -0.53548703, -0.54215832, -0.08426844, -0.52866698, -0.53139835,\n",
       "       -0.26042643, -0.53856146, -0.41938873, -0.08472306, -0.22376852,\n",
       "       -0.46323065, -0.46323065, -0.40293303, -0.54152665, -0.5636839 ,\n",
       "       -0.45488564, -0.08426844, -0.08158473, -0.48548627, -0.08830742,\n",
       "       -0.39223314, -0.14247007, -0.47575616, -0.40443561, -0.08640752,\n",
       "       -0.13730815, -0.08830742, -0.08830742, -0.48229777, -0.47061751,\n",
       "       -0.44572374, -0.52866698, -0.39089571, -0.15703981, -0.16578745,\n",
       "       -0.329113  , -0.17187722, -0.10424399, -0.37669155, -0.11767176,\n",
       "       -0.46383898, -0.43255414, -0.44572374, -0.5503485 , -0.1548253 ,\n",
       "       -0.15900248, -0.18363371, -0.1269284 , -0.16650847, -0.3336542 ,\n",
       "       -0.1269284 , -0.47213685, -0.21772306, -0.46635714, -0.32728796])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ite_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1005)\n",
    "n_train = 100\n",
    "p       = 5\n",
    "n_pool  = 1000\n",
    "n_test  = 1000\n",
    "n2      = 5\n",
    "X_train = np.random.normal(size = n_train*p).reshape((n_train,p))\n",
    "t_train = np.random.binomial(n = 1, p = 0.5, size = n_train)\n",
    "y_train = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_train[:, 1]*2 + t_train*3))))\n",
    "\n",
    "X_pool = np.random.normal(size = n_pool*p).reshape((n_pool,p))\n",
    "t_pool = np.random.binomial(n = 1, p = 0.5, size = n_pool)\n",
    "y_pool = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_pool[:, 1]*2 + t_pool*3))))\n",
    "\n",
    "X_test = np.random.normal(size = n_test*p).reshape((n_test,p))\n",
    "t_test = np.random.binomial(n = 1, p = 0.5, size = n_test)\n",
    "y_test = np.random.binomial(n = 1, p = 1/(1+np.exp(-1*(X_test[:, 1]*2 + t_test*3))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qini before AL: 0.09689246936275164\n",
      "Qini after round 0 of AL: 0.10772043300093255\n",
      "Qini after round 5 of AL: 0.10995790621127836\n",
      "Qini after round 10 of AL: 0.1058947556523674\n",
      "Qini after round 15 of AL: 0.12809717270333737\n",
      "Qini after round 20 of AL: 0.12527320366578015\n"
     ]
    }
   ],
   "source": [
    "asl = ASLearner(estimator = ITEEstimator(model = XBART(),two_model=False), \n",
    "         query_strategy=expected_model_change_maximization,\n",
    "         X_training = X_train,\n",
    "         t_training = t_train,\n",
    "         y_training = y_train,\n",
    "         X_pool     = X_pool,\n",
    "         X_test     = X_test,\n",
    "         approx_model=SGDRegressor())\n",
    "asl.fit()\n",
    "p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "print(\"Qini before AL: {}\".format(asl.score(preds=np.mean(p_ite, axis=1),\n",
    "                                            y_true=y_test, t_true=t_test)))\n",
    "qini_vals = []\n",
    "for batch_round in range(21):\n",
    "    X_query, ix = asl.query(asl.X_pool, n2=n2, return_mean=False)\n",
    "    asl.teach(X_query, t_pool[ix], y_pool[ix])\n",
    "    asl.X_pool = np.delete(asl.X_pool,ix, axis=0)\n",
    "    t_pool     = np.delete(t_pool,ix, axis=0) \n",
    "    y_pool     = np.delete(y_pool,ix, axis=0) \n",
    "    p_ite, p_y1, p_y0 = asl.predict(asl.X_test, return_mean=False)\n",
    "    qini_vals.append(asl.score(preds=np.mean(p_ite, axis=1), y_true=y_test, t_true=t_test))\n",
    "    if batch_round % 5 == 0:\n",
    "        print(\"Qini after round {} of AL: {}\".format(batch_round,qini_vals[batch_round]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
